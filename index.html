<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta property='og:title' content='CVPR2023 MVCS. Machine Visual Common Sense: Perception, Prediction, Planning' />
    <meta property='og:url' content='https://csail.mit.edu/mvcs' />
    <meta property="og:type" content="website" />

    <title>CVPR2023 MVCS</title>

    <!-- link to CSS -->
    <link rel="stylesheet" href="./css/style.css">
    <link rel="stylesheet" href="./css/ibm-plex.min.css">

    <!-- jQuery CSS/Js -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
    <!-- Fontawesome for Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

</head>

<body>
    <div id="header">
        <p>The Workshop on</p>
        <h1 class="gradient" style="font-weight: 600;">Machine Visual Common Sense: Perception, Prediction, Planning</h1>
        <p style="color: #5d5d5d; margin-bottom: 30px;">June 19 @<br><b>CVPR 2023<br>Vancouver Canada</b></p>
        <div id="menu">
            <ul>
                <!--
                <li><a href="https://go-live-il.zoom.us/j/86466508027?pwd=N042ajNJM3NNSWlPeE1ISDJnaTRCUT09#success">Zoom Link</a></li>
                -->
                <li><a href="./#introduction">About</a></li>
                <li><a href="./#challenges">Challenge</a></li>
                <li><a href="./#speakers">Organizers & Speakers</a></li>
                <li><a href="./schedule.html" accesskey="2">Schedule</a></li>
                <li><a href="./#contact">Contact</a></li>
            </ul>
        </div>
        <a class="up" href="#top"><i class="fa fa-arrow-up"></i></a>
    </div>

    <div id="content">
        <div id="top"></div>

        <h1 id="introduction">About</h1>
        <p>
            Over the years, there have been a variety of visual reasoning tasks that evaluate machines’ ability to understand and reason about visual scenes. However, these benchmarks mostly focus on classification of objects and items that exist in a scene. <span style="background: #e39dd0; color: #fff; padding: 0 5px;">Common sense reasoning – an understanding of what might happen next, or what gave rise to the scene</span> – is often absent in these benchmarks. Humans, on the other hand, are highly versatile, adept in numerous high-level cognition-related visual reasoning tasks that go beyond pattern recognition and require common sense (e.g., physics, causality, functionality, psychology, etc).
        </p>
        <p>
            <b>In order to design systems with human-like visual understanding of the world, we would like to emphasize benchmarks and tasks that evaluate common sense reasoning across a variety of domains, including but not limited to:</b>
        </p>
        <ul>
            <li><b>Intuitive Physics:</b><br>A general understanding and expectations about the physical world (e.g., how things support, collide, fall, contain, become unstable etc.)</li>
            <li><b>Intuitive Psychology & Social Science:</b><br>A basic understanding of inter-relations and interaction of agents; An understanding of instrumental actions (e.g., assistance, imitation, speech etc.); The ability to reason about hidden mental variables that drive observable actions.</li>
            <li><b>Affordance & Functionality:</b><br>What actions of agents can be applied to objects; What functions objects provide for the agents.</li>
            <li><b>Causality & Counterfactual Thinking:</b><br>Understanding of causes and effects; Mental representations of alternatives to past or future events, actions, or states.</li>
        </ul>
        
        <hr>
        
        <!--         
        <h1 id="winners">Challenge Winners & Papers</h1>
        Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, Changwen Chen, <a href="https://mvcs-workshop.github.io/data/v1.1-2.pdf">The Lame Can’t Go Far: Visual Stream Limits The Video Question Answering</a><br><br>
        Alice Hein, Klaus Diepold, <a href="https://mvcs-workshop.github.io/data/BIB_Challenge-1.pdf">Winning Solution of the BIB MVCS Challenge 2022</a><br><br>
        Xin Huang, Jung Jae Kim, Hui Li Tan, <a href="https://mvcs-workshop.github.io/data/ECCV_2022_MVCS_STAR_challenge_final.pdf">Comparing classification and generation approaches to situated reasoning with vision-language pre-trained models</a><br><br>
        Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, Animesh Garg, <a href="https://mvcs-workshop.github.io/data/MVCS-SlotFormer.pdf">SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models</a>
        -->
        
        <h1 id="challenges">Challenge Tracks</h1>
        <p>
            There will be <b>two core tracks</b> in the machine vision common sense challenge:
        </p>


        <div class="row">
            <div style="border-color: #540bb3">
                <h3 style="color: #540bb3;">STAR Challenge</h3>
                <p style="color: #540bb3;"><a href="http://star.csail.mit.edu/">STAR</a> (accepted by NeurIPS2021). STAR Benchmark is a novel dataset for Situated Reasoning, which provides challenging question-answering tasks, symbolic situation descriptions and logic-grounded diagnosis via real-world video situations. Reasoning in the real world is not divorced from situations. A key challenge is to capture the present knowledge from surrounding situations and reason accordingly. Situated reasoning is a cognitive process where a person uses context and environmental cues to make decisions and solve problems. </p>
                <p><a class="downloadbtn" target="_blank" href="http://star.csail.mit.edu/#repo">Download Link</a><br><a class="downloadbtn" target="_blank" href="https://eval.ai/web/challenges/challenge-page/1325/leaderboard/3328/Mean">STAR Challenge Evaluation</a></p>
            </div>
            <div style="border-color: #540bb3">
                <h3 style="color: #540bb3;">CLEVRER & ComPhy Challenge</h3>
                <p style="color: #540bb3;"><a href="http://clevrer.csail.mit.edu/">CLEVRER</a> (accepted by ICLR2020) and <a href="https://comphyreasoning.github.io">ComPhy</a> (accepted by ICLR 2022). CLEVRER is a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of question: descriptive (e.g., “what color"), explanatory (”what’s responsible for"), predictive (”what will happen next"), and counterfactual (“what if"). ComPhy takes a step further and requires machines to learn the new compositional visible and hidden physical properties from only a few examples. ComPhy includes three types of questions: factual questions for the composition between visible and hidden physical properties, counterfactual questions on objects’ physical properties like mass and charge, and predictive questions for objects’ future movement.</p>
                <p><a class="downloadbtn" target="_blank" href="http://clevrer.csail.mit.edu/#Dataset">Download Link (CLEVRER)</a><br><a class="downloadbtn" target="_blank" href="https://comphyreasoning.github.io/#dataset">Download Link (ComPhy)</a><br><a class="downloadbtn" target="_blank" href="https://eval.ai/web/challenges/challenge-page/667/overview">Evaluation Server (CLEVRER)</a><br><a class="downloadbtn" target="_blank" href="https://eval.ai/web/challenges/challenge-page/1770/overview">Evaluation Server (ComPhy)</a></p>
            </div>
        </div>
        
        
        <h1 id="speakers">Invited Speakers</h1>
        <div class="orgs">
            <div>
                <a href="https://https://people.eecs.berkeley.edu/~malik" target="_blank">
                    <div style="background-image: url('https://people.eecs.berkeley.edu/~malik/malik-color1.jpg')"></div>
                    <p>Jitendra Malik</p>
                    <label>UC Berkeley</label>
                </a>
            </div>
            <!-- <div>
                <a href="https://www.mit.edu/~jda/" target="_blank">
                    <div style="background-image: url('https://www2.eecs.berkeley.edu/Faculty/Photos/Fullsize/darrell.jpg')"></div>
                    <p>Trevor Darrell</p>
                    <label>UC Berkeley</label>
                </a>
            </div> -->
            <div>
                <a href="https://www.mit.edu/~jda/" target="_blank">
                    <div style="background-image: url('https://www.mit.edu/~jda/figs/head_small.jpg')"></div>
                    <p>Jacob Andreas</p>
                    <label>MIT</label>
                </a>
            </div>
            <div>
                <a href="https://jiasenlu.github.io/" target="_blank">
                    <div style="background-image: url('https://jiasenlu.github.io/images/profile.jpeg')"></div>
                    <p>Jiasen Lu</p>
                    <label>AI2</label>
                </a>
            </div>
            <div>
                <a href="https://rowanzellers.com/" target="_blank">
                    <div style="background-image: url('https://rowanzellers.com/assets/img/me-sm.jpg')"></div>
                    <p>Rowan Zellers</p>
                    <label>Open AI</label>
                </a>
            </div>
            <a>To be continued</a>
            <!-- 
            <div>
                <a href="https://jiajunwu.com" target="_blank">
                    <div style="background-image: url('./data/Jiajun.jpeg')"></div>
                    <p>Jiajun Wu</p>
                    <label>Stanford</label>
                </a>
            </div>
            <div>
                <a href="https://www.csail.mit.edu/person/leslie-kaelbling" target="_blank">
                    <div style="background-image: url('./data/kaelbling.jpeg')"></div>
                    <p>Leslie Kaebling</p>
                    <label>MIT</label>
                </a>
            </div>
            -->

        </div>
        <hr>
        
        
       <h1>Timeline</h1>
        <ul class="timeline">
            <li>
                <p>Workshop</p>
                <label>June 19, 2023 (afternoon)</label>
            </li>
            <li>
                <p>Challenge</p>
                <label>March 25 - May 20, 2023</label>
            </li>
            <li>
                <p>Challenge Submission Deadlines</p>
                <label>May 20, 2023. Check each challenge for the specific date.</label>
            </li>
            <li>
                <p>Notification of Winner & Paper Invitation Deadline</p>
                <label>before June 1, 2023 (Anywhere on Earth)</label>
            </li>
            <!-- 
            <li>
                <p>Paper Submission Deadline. <a href="https://cmt3.research.microsoft.com/MVCSWS2022/">Submission site</a></p>
                <label>XXXX, 2023 (Anywhere on Earth)</label>
            </li>
            -->
        </ul>

        <h1 id="organizers">Organizers</h1>
        <div class="orgs">
            <div>
                <a href="https://evelinehong.github.io/" target="_blank">
                    <div style="background-image: url('./data/yining2.png')"></div>
                    <p>Yining Hong</p>
                    <label>UCLA</label>
                </a>
            </div>
            <div>
                <a href="https://bobbywu.com" target="_blank">
                    <div style="background-image: url('./data/bobby.jpeg')"></div>
                    <p>Bo Wu</p>
                    <label>MIT-IBM Watson AI Lab</label>
                </a>
            </div>
            <div>
                <a href="https://zfchenunique.github.io/" target="_blank">
                    <div style="background-image: url('./data/zfchen.jpg')"></div>
                    <p>Zhenfang Chen</p>
                    <label>MIT-IBM Watson AI Lab</label>
                </a>
            </div>
            <div>
                <a href="https://zhouqqhh.github.io/" target="_blank">
                    <div style="background-image: url('./data/qinhong.jpeg')"></div>
                    <p>Qinhong Zhou</p>
                    <label>Tsinghua University</label>
                </a>
            </div>
            <div>
                <a href="https://dingmyu.github.io" target="_blank">
                    <div style="background-image: url('./data/mingyu.png')"></div>
                    <p>Mingyu Ding</p>
                    <label>UC Berkeley</label>
                </a>
            </div>
            <div>
                <a href="https://people.csail.mit.edu/ganchuang" target="_blank">
                    <div style="background-image: url('./data/chuang.jpeg')"></div>
                    <p>Chuang Gan</p>
                    <label>UMass Amherst and MIT-IBM Watson AI Lab</label>
                </a>
            </div>
        </div>
        <hr>
        
  
        <h1 id="contact">Contact Info</h1>
        <p>E-mail:
            <a href="mailto:yininghong@cs.ucla.edu" target="_blank">yininghong@cs.ucla.edu</a>
        </p>

        <div style="height: 100px;">&nbsp;</div>
    </div>
    <script src="./js/interactions.js"></script>

</body></html>
